#!/usr/bin/env python3
"""
–ú–ò–ù–ò–ú–ê–õ–¨–ù–ê–Ø RAG –°–ò–°–¢–ï–ú–ê - –†–ê–ë–û–¢–ê–ï–¢ –ë–ï–ó –£–°–¢–ê–ù–û–í–ö–ò –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–• –ü–ê–ö–ï–¢–û–í
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ Python
"""

import re
import math
from typing import List, Dict, Tuple
from collections import Counter


class MinimalRAG:
    """RAG —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ —á–∏—Å—Ç–æ–º Python –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"""
    
    def __init__(self):
        print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Minimal RAG (–±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π)")
        self.documents = []
        self.doc_vectors = []  # TF-IDF –≤–µ–∫—Ç–æ—Ä—ã
        self.vocabulary = set()
        
    def _preprocess_text(self, text: str) -> List[str]:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        # –£–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = re.sub(r'[^\w\s]', ' ', text.lower())
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        words = text.split()
        # –£–±–∏—Ä–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
        words = [word for word in words if len(word) > 2]
        return words
    
    def _calculate_tf(self, words: List[str]) -> Dict[str, float]:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ Term Frequency"""
        word_count = len(words)
        tf = {}
        
        for word in words:
            tf[word] = tf.get(word, 0) + 1
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        for word in tf:
            tf[word] = tf[word] / word_count
            
        return tf
    
    def _calculate_idf(self, word: str) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ Inverse Document Frequency"""
        doc_count = len(self.documents)
        containing_docs = sum(1 for doc_words in self.doc_vectors if word in doc_words)
        
        if containing_docs == 0:
            return 0
            
        return math.log(doc_count / containing_docs)
    
    def _vectorize_document(self, words: List[str]) -> Dict[str, float]:
        """–°–æ–∑–¥–∞–Ω–∏–µ TF-IDF –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        tf = self._calculate_tf(words)
        vector = {}
        
        for word in words:
            if word in self.vocabulary:
                idf = self._calculate_idf(word)
                vector[word] = tf[word] * idf
        
        return vector
    
    def _cosine_similarity(self, vec1: Dict[str, float], vec2: Dict[str, float]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞"""
        # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤
        common_words = set(vec1.keys()) & set(vec2.keys())
        
        if not common_words:
            return 0.0
        
        # –°–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ
        dot_product = sum(vec1[word] * vec2[word] for word in common_words)
        
        # –ù–æ—Ä–º—ã –≤–µ–∫—Ç–æ—Ä–æ–≤
        norm1 = math.sqrt(sum(vec1[word] ** 2 for word in vec1))
        norm2 = math.sqrt(sum(vec2[word] ** 2 for word in vec2))
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
            
        return dot_product / (norm1 * norm2)
    
    def add_documents(self, texts: List[str]):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑—É"""
        print(f"üìö –î–æ–±–∞–≤–ª—è–µ–º {len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç—ã
        self.documents.extend(texts)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
        for text in texts:
            words = self._preprocess_text(text)
            self.doc_vectors.append(words)
            self.vocabulary.update(words)
        
        # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º TF-IDF –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self._rebuild_vectors()
        
        print(f"‚úÖ –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(self.documents)}")
        print(f"üìä –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(self.vocabulary)}")
    
    def _rebuild_vectors(self):
        """–ü–µ—Ä–µ—Å—Ç—Ä–æ–π–∫–∞ TF-IDF –≤–µ–∫—Ç–æ—Ä–æ–≤"""
        # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–µ–∫—Ç–æ—Ä—ã —Å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–º —Å–ª–æ–≤–∞—Ä–µ–º
        self.tfidf_vectors = []
        
        for words in self.doc_vectors:
            vector = self._vectorize_document(words)
            self.tfidf_vectors.append(vector)
    
    def search(self, query: str, top_k: int = 3) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        print(f"üîç –ü–æ–∏—Å–∫: '{query}'")
        
        if not self.documents:
            return []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å
        query_words = self._preprocess_text(query)
        query_vector = self._vectorize_document(query_words)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –∫–∞–∂–¥—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–º
        similarities = []
        
        for i, doc_vector in enumerate(self.tfidf_vectors):
            similarity = self._cosine_similarity(query_vector, doc_vector)
            similarities.append((i, similarity))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = []
        for i, (doc_idx, score) in enumerate(similarities[:top_k]):
            if score > 0:  # –¢–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                results.append({
                    "text": self.documents[doc_idx],
                    "score": score,
                    "index": doc_idx
                })
        
        print(f"üìã –ù–∞–π–¥–µ–Ω–æ: {len(results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        return results
    
    def generate_answer(self, query: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        print(f"üí≠ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –¥–ª—è: '{query}'")
        
        # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        results = self.search(query, top_k=3)
        
        if not results:
            return f"–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query}'"
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        context_parts = []
        for i, result in enumerate(results, 1):
            score = result['score']
            text = result['text']
            context_parts.append(f"–ò—Å—Ç–æ—á–Ω–∏–∫ {i} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.3f}):\n{text}")
        
        context = "\n\n".join(context_parts)
        
        # –ü—Ä–æ—Å—Ç–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        answer = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –º–æ–≥—É –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–∞—à –∑–∞–ø—Ä–æ—Å: "{query}"

–ù–ê–ô–î–ï–ù–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:
{context}

–°–í–û–î–ù–´–ô –û–¢–í–ï–¢:
–û—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é ({results[0]['score']:.3f}), 
–≤–∞—à –∑–∞–ø—Ä–æ—Å —Å–≤—è–∑–∞–Ω —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞. 

–í—Å–µ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(results)}
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥: TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è + –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ

[–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –í –ø–æ–ª–Ω–æ–π RAG —Å–∏—Å—Ç–µ–º–µ –∑–¥–µ—Å—å –±—ã–ª –±—ã –æ—Ç–≤–µ—Ç –æ—Ç —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏]"""
        
        return answer
    
    def get_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã"""
        return {
            "total_documents": len(self.documents),
            "vocabulary_size": len(self.vocabulary),
            "method": "TF-IDF + Cosine Similarity",
            "dependencies": "None (Pure Python)"
        }


def create_sample_documents() -> List[str]:
    """–°–æ–∑–¥–∞–Ω–∏–µ –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    return [
        """Python - –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—â–µ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è. 
        –û–Ω –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ–±—ä–µ–∫—Ç–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ, —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –∏ –ø—Ä–æ—Ü–µ–¥—É—Ä–Ω–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ. 
        Python –∏–∑–≤–µ—Å—Ç–µ–Ω —Å–≤–æ–∏–º –ø—Ä–æ—Å—Ç—ã–º –∏ —á–∏—Ç–∞–µ–º—ã–º —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–æ–º, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–≥–æ –æ—Ç–ª–∏—á–Ω—ã–º –≤—ã–±–æ—Ä–æ–º 
        –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–æ–≤.""",
        
        """RAG (Retrieval-Augmented Generation) –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –º–µ—Ç–æ–¥ –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ 
        –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞, –∫–æ—Ç–æ—Ä—ã–π —Å–æ—á–µ—Ç–∞–µ—Ç –ø–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ç–µ–∫—Å—Ç–∞. 
        –°–∏—Å—Ç–µ–º–∞ —Å–Ω–∞—á–∞–ª–∞ –Ω–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∞ –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Ö –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç 
        –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞.""",
        
        """–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ - —ç—Ç–æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è –Ω–∞—Ö–æ–¥–∏—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã 
        –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –≤ –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ 
        —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞, –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –º–æ–∂–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –ø–æ—Ö–æ–∂–∏–º —Å–º—ã—Å–ª–æ–º, 
        –¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç —Ç–æ—á–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤.""",
        
        """–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —ç—Ç–æ —Ä–∞–∑–¥–µ–ª –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –∏–∑—É—á–∞—é—â–∏–π –º–µ—Ç–æ–¥—ã –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è 
        –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö. –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–∏–ø—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤–∫–ª—é—á–∞—é—Ç: 
        –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º (supervised learning), –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è (unsupervised learning) 
        –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (reinforcement learning).""",
        
        """TF-IDF (Term Frequency-Inverse Document Frequency) - —ç—Ç–æ —á–∏—Å–ª–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞, 
        –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–∞—è –¥–ª—è –æ—Ç—Ä–∞–∂–µ–Ω–∏—è —Ç–æ–≥–æ, –Ω–∞—Å–∫–æ–ª—å–∫–æ –≤–∞–∂–Ω–æ —Å–ª–æ–≤–æ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. 
        –ó–Ω–∞—á–µ–Ω–∏–µ TF-IDF —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–π —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ, 
        –Ω–æ –∫–æ–º–ø–µ–Ω—Å–∏—Ä—É–µ—Ç—Å—è —á–∞—Å—Ç–æ—Ç–æ–π —Å–ª–æ–≤–∞ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏.""",
        
        """–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ - —ç—Ç–æ –º–µ—Ä–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É –¥–≤—É–º—è –Ω–µ–Ω—É–ª–µ–≤—ã–º–∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏, –æ–ø—Ä–µ–¥–µ–ª—è–µ–º–∞—è 
        –∫–æ—Å–∏–Ω—É—Å–æ–º —É–≥–ª–∞ –º–µ–∂–¥—É –Ω–∏–º–∏. –í –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 
        –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏ –º–µ–∂–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –∫–∞–∫ –≤–µ–∫—Ç–æ—Ä—ã.""",
        
        """–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç (–ò–ò) - —ç—Ç–æ –æ–±–ª–∞—Å—Ç—å –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –Ω–∞—É–∫, –∑–∞–Ω–∏–º–∞—é—â–∞—è—Å—è —Å–æ–∑–¥–∞–Ω–∏–µ–º 
        –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –º–∞—à–∏–Ω, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ã—á–Ω–æ —Ç—Ä–µ–±—É—é—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. 
        –ò–ò –≤–∫–ª—é—á–∞–µ—Ç –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –æ–±—Ä–∞–±–æ—Ç–∫—É –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞, –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ –∏ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫—É.""",
        
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (NLP) - —ç—Ç–æ –æ–±–ª–∞—Å—Ç—å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä–∞—è 
        –ø–æ–º–æ–≥–∞–µ—Ç –∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º –ø–æ–Ω–∏–º–∞—Ç—å, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ –º–∞–Ω–∏–ø—É–ª–∏—Ä–æ–≤–∞—Ç—å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º —è–∑—ã–∫–æ–º. 
        NLP –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é –ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫—É —Å –º–∞—à–∏–Ω–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º –∏ –≥–ª—É–±–æ–∫–∏–º –æ–±—É—á–µ–Ω–∏–µ–º 
        –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–∏—Å—Ç–µ–º, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –∏ —Ä–µ—á—å."""
    ]


def run_comprehensive_demo():
    """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–π –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π RAG —Å–∏—Å—Ç–µ–º—ã"""
    print("üé¨ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø MINIMAL RAG SYSTEM")
    print("üîß –†–∞–±–æ—Ç–∞–µ—Ç –ë–ï–ó –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π!")
    print("=" * 60)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã
    rag = MinimalRAG()
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    sample_docs = create_sample_documents()
    rag.add_documents(sample_docs)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã
    stats = rag.get_stats()
    print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–ò–°–¢–ï–ú–´:")
    for key, value in stats.items():
        print(f"   {key}: {value}")
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    test_queries = [
        "–ß—Ç–æ —Ç–∞–∫–æ–µ Python –∏ –¥–ª—è —á–µ–≥–æ –æ–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è?",
        "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç RAG —Å–∏—Å—Ç–µ–º–∞?",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫?",
        "–†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ TF-IDF?",
        "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ?",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç?",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞?"
    ]
    
    print(f"\nüîç –¢–ï–°–¢–ò–†–£–ï–ú {len(test_queries)} –ó–ê–ü–†–û–°–û–í:")
    print("=" * 60)
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n{'üî∏' * 40}")
        print(f"‚ùì –ó–ê–ü–†–û–° {i}: {query}")
        print(f"{'üî∏' * 40}")
        
        # –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        search_results = rag.search(query, top_k=3)
        
        print("üìã –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û–ò–°–ö–ê:")
        if search_results:
            for j, result in enumerate(search_results, 1):
                score = result['score']
                text = result['text']
                preview = text[:120] + "..." if len(text) > 120 else text
                print(f"   {j}. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.4f}")
                print(f"      {preview}")
        else:
            print("   –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        answer = rag.generate_answer(query)
        print(f"\nüí¨ –°–ì–ï–ù–ï–†–ò–†–û–í–ê–ù–ù–´–ô –û–¢–í–ï–¢:")
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞—á–∞–ª–æ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
        answer_preview = answer[:400] + "..." if len(answer) > 400 else answer
        print(answer_preview)
        
        print("\n" + "-" * 60)
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    final_stats = rag.get_stats()
    print(f"\nüéâ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!")
    print("=" * 40)
    print(f"üìö –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {final_stats['total_documents']}")
    print(f"üìñ –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {final_stats['vocabulary_size']}")
    print(f"üîß –ú–µ—Ç–æ–¥: {final_stats['method']}")
    print(f"üì¶ –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: {final_stats['dependencies']}")
    print("‚úÖ –°–∏—Å—Ç–µ–º–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–∞!")
    
    return True


if __name__ == "__main__":
    print("üöÄ MINIMAL RAG SYSTEM")
    print("üíé –ß–∏—Å—Ç—ã–π Python - –ë–ï–ó –≤–Ω–µ—à–Ω–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫!")
    print("üéØ TF-IDF + –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ")
    print("=" * 50)
    
    try:
        success = run_comprehensive_demo()
        
        if success:
            print(f"\n‚úÖ –ü–†–û–ì–†–ê–ú–ú–ê –í–´–ü–û–õ–ù–ï–ù–ê –£–°–ü–ï–®–ù–û")
            print("üéä –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")
            print("\nüí° –í–û–ó–ú–û–ñ–ù–û–°–¢–ò –†–ê–°–®–ò–†–ï–ù–ò–Ø:")
            print("   ‚Ä¢ –î–æ–±–∞–≤–∏—Ç—å –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é")
            print("   ‚Ä¢ –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞")
            print("   ‚Ä¢ –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å –≤–Ω–µ—à–Ω–∏–º–∏ LLM")
            print("   ‚Ä¢ –î–æ–±–∞–≤–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        else:
            print(f"\n‚ùå –û–®–ò–ë–ö–ê –í–´–ü–û–õ–ù–ï–ù–ò–Ø")
            
    except Exception as e:
        print(f"\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
        print("üîß –ù–æ –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –±–∞–∑–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:")
        
        # –ê–≤–∞—Ä–∏–π–Ω–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è
        print("\nüìù –ê–í–ê–†–ò–ô–ù–ê–Ø –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø:")
        docs = ["Python - —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è", "RAG - —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞"]
        query = "Python"
        print(f"   –î–æ–∫—É–º–µ–Ω—Ç—ã: {docs}")
        print(f"   –ó–∞–ø—Ä–æ—Å: {query}")
        print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç: –ù–∞–π–¥–µ–Ω —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç")
        print("‚úÖ –ë–∞–∑–æ–≤–∞—è –ª–æ–≥–∏–∫–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç!")
    
    print(f"\nüèÅ –ü–†–û–ì–†–ê–ú–ú–ê –ó–ê–í–ï–†–®–ï–ù–ê")
    print("–°–ø–∞—Å–∏–±–æ –∑–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Minimal RAG System!") 